<!DOCTYPE html>
<html>

<head>

    <!-- Document Settings -->
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />

    <!-- jQuery + Fitvids, which makes all video embeds responsive -->
    <script src="https://code.jquery.com/jquery-3.2.1.min.js"
        integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous">
        </script>
    <script type="text/javascript" src="/assets/js/jquery.fitvids.js"></script>
    <!-- highlight.js -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.10.0/components/prism-abap.min.js"></script>
    <script>$(document).ready(function () {
            $('pre code').each(function (i, block) {
                hljs.highlightBlock(block);
            });
        });</script>

    <!-- Paginator increased to "infinit" in _config.yml -->
    <!-- if paginator.posts  -->
    <!-- <script>
        var maxPages = parseInt('');
    </script>
    <script src="/assets/js/infinitescroll.js"></script> -->
    <!-- /endif -->

    


    <!-- Base Meta -->
    <!-- dynamically fixing the title for tag/author pages -->


    <title>Indonesian Language Model</title>
    <meta name="HandheldFriendly" content="True" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <!-- Styles'n'Scripts -->
    <link rel="stylesheet" type="text/css" href="/assets/built/screen.css" />
    <link rel="stylesheet" type="text/css" href="/assets/built/screen.edited.css" />
    <link rel="stylesheet" type="text/css" href="/assets/built/syntax.css" />
    <!-- highlight.js -->
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css">
    <style>
        .hljs {
            background: none;
        }
    </style>

    <!--[if IE]>
        <style>
            p, ol, ul{
                width: 100%;
            }
            blockquote{
                width: 100%;
            }
        </style>
    <![endif]-->

    <!-- This tag outputs SEO meta+structured data and other important settings -->
    <meta name="description"
    content="Wrangling about NLP, Machine Learning, and Personal Growth" />
<link rel="shortcut icon" href="favicon.png" type="image/png" />
<link rel="canonical"
    href="http://localhost:4000/indonesian-language-model" />
<meta name="referrer" content="no-referrer-when-downgrade" />


<!--title below is coming from _includes/dynamic_title-->
<meta property="og:site_name" content="Aditya Kurniawan" />
<meta property="og:type" content="website" />
<meta property="og:title"
    content="Indonesian Language Model" />
<meta property="og:description"
    content="Lingua NLP (Natural Language Processing) has been proven useful for many industrial practitioners to gain insight and automate human-intensive labor in order to bring a better experience for their customers. Chatbot to quickly reply customers’ inquiries and free text search engine to help customers express their intent towards our product" />
<meta property="og:url"
    content="http://localhost:4000/indonesian-language-model" />
<meta property="og:image"
    content="http://localhost:4000/assets/images/posts/language-model/awd-arch.png" />
<meta property="article:publisher" content="https://www.facebook.com/" />
<meta property="article:author" content="https://www.facebook.com/" />
<meta property="article:published_time"
    content="2018-12-11T00:00:00+07:00" />
<meta property="article:modified_time"
    content="2018-12-11T00:00:00+07:00" />

<meta property="article:tag" content="Advanced-nlp" />
<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:title"
    content="Indonesian Language Model" />
<meta name="twitter:description"
    content="Lingua NLP (Natural Language Processing) has been proven useful for many industrial practitioners to gain insight and automate human-intensive labor in order to bring a better experience for their customers. Chatbot to quickly reply customers’ inquiries and free text search engine to help customers express their intent towards our product" />
<meta name="twitter:url" content="http://localhost:4000/" />
<meta name="twitter:image"
    content="http://localhost:4000/assets/images/posts/language-model/awd-arch.png" />
<meta name="twitter:label1" content="Written by" />
<meta name="twitter:data1" content="Aditya Kurniawan" />
<meta name="twitter:label2" content="Filed under" />
<meta name="twitter:data2" content="Advanced-nlp" />
<meta name="twitter:site" content="@akurnia_1" />
<meta name="twitter:creator" content="@akurnia_1" />
<meta property="og:image:width" content="1400" />
<meta property="og:image:height" content="933" />

<script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Website",
    "publisher": {
        "@type": "Organization",
        "name": "Aditya Kurniawan",
        "logo": "http://localhost:4000/"
    },
    "url": "http://localhost:4000/indonesian-language-model",
    "image": {
        "@type": "ImageObject",
        "url": "http://localhost:4000/assets/images/posts/language-model/awd-arch.png",
        "width": 2000,
        "height": 666
    },
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "http://localhost:4000/indonesian-language-model"
    },
    "description": "Lingua NLP (Natural Language Processing) has been proven useful for many industrial practitioners to gain insight and automate human-intensive labor in order to bring a better experience for their customers. Chatbot to quickly reply customers’ inquiries and free text search engine to help customers express their intent towards our product"
}
</script>

<!-- <script type="text/javascript" src="https://demo.ghost.io/public/ghost-sdk.min.js?v=724281a32e"></script>
    <script type="text/javascript">
    ghost.init({
    	clientId: "ghost-frontend",
    	clientSecret: "f84a07a72b17"
    });
    </script> -->

<meta name="generator" content="Jekyll 3.6.2" />
<link rel="alternate" type="application/rss+xml"
    title="Indonesian Language Model"
    href="/feed.xml" />

</head>

<body
    class="post-template">

    <div class="site-wrapper">
        <!-- All the main content gets inserted here, index.hbs, post.hbs, etc -->
        <!-- default -->

<!-- The tag above means: insert everything in this file
into the {body} of the default.hbs template -->

<header class="site-header outer">
    <div class="inner">
        <nav class="site-nav">
    <div class="site-nav-left">
        
        
        <a class="site-nav-logo" href="http://localhost:4000/">Aditya Kurniawan</a>
        
        
        
        <ul class="nav" role="menu">
    <li class="nav-home" role="menuitem"><a href="/">Home</a></li>
    <li class="nav-about" role="menuitem"><a href="/about">About</a></li>
    <!-- <li class="nav-getting-started" role="menuitem"><a href="/tag/getting-started/">Getting Started</a></li>
    <li class="nav-try-ghost" role="menuitem"><a href="https://ghost.org">Try Ghost</a></li> -->
</ul>
        
    </div>
    <!-- <div class="site-nav-right">
        <div class="social-links">
            
            
            <a class="social-link social-link-tw" href="https://twitter.com/akurnia_1" target="_blank"
                rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32">
    <path
        d="M30.063 7.313c-.813 1.125-1.75 2.125-2.875 2.938v.75c0 1.563-.188 3.125-.688 4.625a15.088 15.088 0 0 1-2.063 4.438c-.875 1.438-2 2.688-3.25 3.813a15.015 15.015 0 0 1-4.625 2.563c-1.813.688-3.75 1-5.75 1-3.25 0-6.188-.875-8.875-2.625.438.063.875.125 1.375.125 2.688 0 5.063-.875 7.188-2.5-1.25 0-2.375-.375-3.375-1.125s-1.688-1.688-2.063-2.875c.438.063.813.125 1.125.125.5 0 1-.063 1.5-.25-1.313-.25-2.438-.938-3.313-1.938a5.673 5.673 0 0 1-1.313-3.688v-.063c.813.438 1.688.688 2.625.688a5.228 5.228 0 0 1-1.875-2c-.5-.875-.688-1.813-.688-2.75 0-1.063.25-2.063.75-2.938 1.438 1.75 3.188 3.188 5.25 4.25s4.313 1.688 6.688 1.813a5.579 5.579 0 0 1 1.5-5.438c1.125-1.125 2.5-1.688 4.125-1.688s3.063.625 4.188 1.813a11.48 11.48 0 0 0 3.688-1.375c-.438 1.375-1.313 2.438-2.563 3.188 1.125-.125 2.188-.438 3.313-.875z" />
    </svg></a>
            
        </div>
        
    </div> -->
</nav>
    </div>
</header>

<!-- Everything inside the #post tags pulls data from the post -->
<!-- #post -->

<main id="site-main" class="site-main outer" role="main">
    <div class="inner">

        <article
            class="post-full  tag-advanced-nlp tag-language-model tag-representation-learning post ">

            <header class="post-full-header">
                <section class="post-full-meta">
                    <time class="post-full-meta-date"
                        datetime="11 December 2018">11 December 2018</time>
                    
                    <span class="date-divider">/</span>
                    
                    
                    <a href='/tag/advanced-nlp/'>ADVANCED-NLP</a>,
                    
                    
                    
                    <a href='/tag/language-model/'>LANGUAGE-MODEL</a>,
                    
                    
                    
                    <a href='/tag/representation-learning/'>REPRESENTATION-LEARNING</a>
                    
                    
                    
                </section>
                <h1 class="post-full-title">Indonesian Language Model</h1>
            </header>

            
            <figure class="post-full-image" style="background-image: url(/assets/images/posts/language-model/awd-arch.png)">
            </figure>
            

            <section class="post-full-content">
                <div class="kg-card-markdown">
                    <h1 id="lingua">Lingua</h1>
<p>NLP (Natural Language Processing) has been proven useful for many industrial practitioners to gain insight and automate human-intensive labor in order to bring a better experience for their customers. Chatbot to quickly reply customers’ inquiries and free text search engine to help customers express their intent towards our product in more flexible way are a few examples of the use cases.</p>

<p>When dealing with text data, the representation of the text itself is one of the central components to build NLP applications. Recent state-of-the-art of text representations have been powered by word2vec and its variants that were first popularised by <a href="https://arxiv.org/abs/1301.3781">Mikolov et al. 2013</a>. However, as an Indonesian-based technology company, Traveloka deals with a high volume of Indonesian text. Due to the fact that Indonesian is considered as one of the low-resource languages, not much work has been done in text representation for this language.</p>

<h2 id="language-model">Language Model</h2>
<p>It is tempting for us to build word2vec embedding from our own corpus that later can be used by the whole company. However, recent study from <a href="https://arxiv.org/pdf/1804.09692.pdf">Wendlandt et al.2018</a> and <a href="https://mimno.infosci.cornell.edu/papers/antoniak-stability.pdf">Antoniak et al.2018</a> show that word dense representations may suffer from instabilities measured by their closeness of their neighbours. An interesting fact coming from <a href="https://arxiv.org/pdf/1804.09692.pdf">Wendlandt et al.2018</a> states that the instabilities do not affect most of the downstream tasks and shows that LSTM is quite robust to handle the instabilities. They measure it by training an LSTM model in POS Tagging task with pre-trained word embedding that shows instabilities. Coming from those studies, we are looking deeper to the representation resulted from the LSTM model instead of the embedding itself. Our reasoning is strengthened further by <a href="https://arxiv.org/abs/1801.06146">Howard, J and Ruder, S., 2018</a> with their new technique of transfer learning for classification task. Their new framework works by training the LSTM + word embedding in language model fashion in large size corpus such as wikipedia, transfer the trained weight into specific corpus and train the classifier as the final steps.</p>

<p><img src="/assets/images/posts/language-model/awd-arch.png" alt="image-title-here" class="center-image" /></p>

<p>Following <a href="https://arxiv.org/abs/1801.06146">Howard, J and Ruder, S., 2018</a>, we chose <a href="https://arxiv.org/abs/1708.02182">AWD LSTM</a> (without further fine-tuning and cache pointer) as the current state-of-the-art in language model. The first step of ULMFiT is to build a language model trained from generic corpus such as wikipedia and then later to transfer the weight for more specific corpora. However, due to the lack of baseline, we decided to divide the experiment into several phase:
Building the language model directly on our conversational dataset.
Training it on Indonesian wikipedia and transferring the weight to train the model with our conversational dataset.
The goal of this experiment is to compare the significance of transfer learning with the one that is trained from scratch.</p>

<p>Furthermore, it is common practice in NLP to use pre-trained word embedding to improve accuracy on several tasks. In this experiment we will use fasttext as our pre-trained word embedding for language modelling task.</p>

<h3 id="awd-lstm">AWD LSTM</h3>
<p>The authors of AWD LSTM paper propose some optimization and regularization techniques which empirically improve the performance of LSTM language models:</p>

<h4 id="nt-asgd">NT-ASGD</h4>
<p>While momentum SGD is usually better than traditional SGD for training deep neural networks in general, for neural language modeling task, traditional SGD is better than momentum SGD and other algorithms. One variant of it is Averaged SGD (ASGD) which is the same as SGD until some iteration T, but in the following iterations ASGD will update the weights with the average of weight updates of some previous iterations.</p>

<p>NT-ASGD is similar to ASGD, but instead of always using the average to update the weights, it will only do it when the validation metric fails to improve for multiple cycles. In other cases NT-ASGD will be the same as traditional SGD.</p>

<h4 id="weight-dropped-lstm">Weight-dropped LSTM</h4>
<p>Dropout is a popular choice of regularization technique for neural networks. However, it cannot be used for LSTM when an optimized black box LSTM implementation, such as NVIDIA’s cuDNN LSTM, is used. To solve that, DropConnect can be used as an alternative of Dropout. While Dropout randomly set activation units to zero, DropConnect randomly set connection weights to zero. In this way, the dropout operation can be applied once to the weight matrices, before the forward and backward pass. So, any black box LSTM implementation can be used.</p>

<p>Similar to variational dropout, the same individual dropped weights remain dropped over all timesteps, i.e. the entirety of forward and backward pass.</p>

<h4 id="variational-dropout">Variational Dropout</h4>
<p>Variational Dropout samples a binary dropout mask once and then it is used for all repeated connections within the entirety of forward and backward pass. Variational Dropout is not only used for LSTM weights (as described in section (2) before), but also used for all dropout connections. For example, the same dropout masks are used for all inputs and outputs of the LSTM.</p>

<h4 id="embedding-dropout">Embedding Dropout</h4>
<p>This technique is equivalent to performing variational dropout on the connection between the one-hot embedding and the embedding lookup.
Variable-length backpropagation sequences
Instead of having a fixed length of backpropagation sequences, the sequence length is randomly selected for each entirety of forward and backward pass. The learning rate is then rescaled depending on the length of the resulting sequence.</p>

<h4 id="weight-tying">Weight tying</h4>
<p>One way to reduce the number of model’s parameters is to share the weights between the embedding and the softmax layer after the LSTM. This technique is empirically shown to have a better performance than that of the standard LSTM language model.</p>

<h4 id="independent-embedding-size-and-hidden-size">Independent embedding size and hidden size</h4>
<p>In contrast to most LSTM language models, AWD LSTM model uses different sizes for embedding and hidden states.</p>

<h4 id="l2-regularization">L2 Regularization</h4>
<p>L2 regularization is used on the individual unit activations and on the difference in outputs of an RNN at different timesteps.</p>

<h2 id="data-gathering">Data Gathering</h2>
<h3 id="conversational-dataset">Conversational Dataset</h3>
<p>One of the greatest things about training language model is that even though the algorithm still fall into supervised machine learning model, we can gather the label without strenuous effort.</p>

<p>In this phase we decided to work with our conversational data, i.e. the conversation between our customer service agents and our customers, to build the language model as a supporting building block for our other project.</p>

<p>As we may well aware, dealing with user generated data is much more complex as we have more sparse data compared to a standard NLP corpora such as Wikipedia, WSJ, and so on. It is due to typos, abbreviation, and excessive uses of punctuations. To give an example, word “pembayaran” (“payment”) is registered in our corpora in 30 different forms (“pembyaran”, “pembayran”, “pembyran”, “pembayan”, “pembayaraan”, ….).</p>

<p>Before showing you the distribution of our data, we preprocessed our data using some scripts that we build in house. Some of them used to normalising the characters, converting some entities that are easily detected using simple regexes such as email, phone number, and url to their respective unique tokens (EMAIL, PHONE, URL, etc), normalising punctuations, and normalising some abbreviations.</p>

<p>The following is the distribution of our unique tokens from 10,000 feet:</p>

<p><img src="/assets/images/posts/language-model/conversation-dist.png" alt="image-title-here" class="center-image" height="200px" width="330px" /></p>

<p>As you can see from the table above, initially our data consist of 1.9M++ unique tokens. After looking in the data further, we find out that more than half of our unique tokens are actually some “magic” numbers, typos, and other magical forms that we may not even understand without further context. From there we decided to remove the tokens with frequency less than 5 and 10 to reduce the number of word embedding matrix.</p>

<h3 id="wikipedia">Wikipedia</h3>
<p>We use wikimedia to download the latest version of Indonesian wikipedia dataset for our training data. The dumped XML data that is then extracted using <a href="https://github.com/attardi/wikiextractor">wikiextractor</a>, we tried to use Wikipedia parsing from gensim, but the result was not as good.</p>

<p>After the data has been extracted, we follow the work of <a href="http://arxiv.org/abs/1609.07843">WikiText</a> by relying on MosesTokenizer to tokenize and normalize the words. We further tune the preprocessing by doing minimal work to remove unintended strings.</p>

<p>Below is the distribution of the wikipedia dataset that we use:</p>

<p><img src="/assets/images/posts/language-model/wiki-dist.png" alt="image-title-here" class="center-image" /></p>

<h2 id="language-model-training">Language Model Training</h2>
<p>To train language model, we prepare three different versions of a single dataset as cross validation datasets. Cross-validation is an important step for training machine-learning models in order to measure their stability towards the dataset. We generated the three different datasets once upfront and train it using different type of hyperparameters. We are currently measuring the performance through the validation dataset and not yet setting up a golden set. The way we measure the stability of the model is by analyzing the variance of the model performance throughout those chunk of datasets.</p>

<p>Training the language model, especially with a significant amount of vocabularies requires a lot of computational resources while calculating the softmax. In our implementation, we are using <a href="https://arxiv.org/abs/1609.04309">Adaptive Softmax</a> to cut off the cost of the calculation and hence speeding up the training process and safe a number of GPU memory. Further, we train the model with a single instance of 2 P100 GPUs and run every batch steps on those GPUs in parallel. The best model was obtained after 6 days of training on our conversational dataset and around 10 days ++ for our wikipedia dataset. Afterward, the model started to overfit by showing a decreasing performance on validation set. One note that we want to convey is that in this moment, we have not yet tried hyperparameter tuning due to time constraints and still relying on default parameters provided on <a href="https://arxiv.org/abs/1708.02182">AWD LSTM paper</a>.</p>

<p>We evaluated our models based on two different metrics, cross-entropy loss and perplexity. Different from other tasks such as classification where we can measure the accuracy through F1 Score, precision and recall, we measure the performance of language model from the cross-entropy loss directly. Perplexity is essentially the probability that assigned by the language model (from softmax layer),  normalized by the number of words.</p>

<p><img src="/assets/images/posts/language-model/ppl-form.png" alt="image-title-here" class="center-image" /></p>

<p>For brevity, we hereby give you the best scores of a set of experiments that we ran. All of the following scores are given based on the aforementioned validation datasets.</p>

<p>Experiment on Conversational dataset</p>

<p><img src="/assets/images/posts/language-model/word-lm-conv-result.png" alt="image-title-here" class="center-image" /></p>

<p>Experiment on id-wikipedia</p>

<p><img src="/assets/images/posts/language-model/word-lm-wiki-result.png" alt="image-title-here" class="center-image" /></p>

<p>During the training process, we made the following observations:</p>
<ol>
  <li>Evidently, increasing the minimum number of words boost our model by reducing both the loss and perplexity by 0.04. This explains the importance of removing low-frequency words from dictionary.</li>
  <li>We also observe the uses of word embedding (from fastText) leads to a lower performance.</li>
  <li>AWD-LSTM performs consistently across the board. It varies quite a bit on the perplexity side, but not that differ from loss point of view.</li>
</ol>

<p>All in all, AWD LSTM shows a consistent performance and shows stability in two completely different datasets (conversational and id-wikipedia dataset) for low-resource language. This marks a positive result in our effort to create a high-quality language model that will be used as a baseline to our transfer learning experiments later.</p>

<h2 id="future-works">Future Works</h2>
<p>It is true that we are still facing a big size of vocabularies that is introduced by typos in our corpus, but worry not we are on our way to attack that problem. As a snippet, here we are giving you the performance of latest experiment</p>

<p>Experiment on Conversational dataset</p>

<p><img src="/assets/images/posts/language-model/char-lm-conv-result.png" alt="image-title-here" class="center-image" /></p>

<p>We managed to improve the loss by 0.1 and down to 2 points in perplexity.</p>

<h2 id="final-thoughts">Final Thoughts</h2>
<p>During this experiment, here are a few takeaways that might be helpful for other practitioners.</p>

<p>First, be diligent on your data, especially in natural language problem. The real word data may introduce more problems than you anticipate, especially if you decide to tackle the problem in word level manner. You may need to invest more in pre-processing to clean up the data. It may seem common in any NLP tasks, but we can’t stress it enough the importance of handling typos inside your corpus. The other way around for handling typo is by modifying your deep learning architecture and let the model handle the typos by itself.</p>

<p>Second, cross validation is important for machine learning practitioners to understand as a way to measure the stability of your model. Though in deep learning case it is quite expensive to run cross validation, you can always reduce the number of experiments to run so that it fits your budget.</p>

<p>Lastly, training in parallel is one of the important key to train a big deep learning model. In this case, we gained a speed-up of 2x the normal training time by using 2 GPUs.</p>


                </div>
            </section>

            <!-- Email subscribe form at the bottom of the page -->
            

            <footer class="post-full-footer">
                <!-- Everything inside the #author tags pulls data from the author -->
                <!-- #author-->
                
                
                <section class="author-card">
                    
                    <img class="author-profile-image" src="/assets/images/ghost.png"
                        alt="akurniawan" />
                    
                    <section class="author-card-content">
                        <h4 class="author-card-name"><a
                                href="/author/akurniawan">Aditya Kurniawan</a></h4>
                        
                        <p>Student in Erasmus Mundus program. Will start studying in University of Malta and move to Charles University in Pragueu for my second year. Heavily invested in understanding better about NLP and language in general.</p>
                        
                    </section>
                </section>
                <div class="post-full-footer-right">
                    <a class="author-card-button" href="/author/akurniawan">Read More</a>
                </div>
                
                
                <!-- /author  -->
            </footer>

            <!-- If you use Disqus comments, just uncomment this block.
            The only thing you need to change is "test-apkdzgmqhj" - which
            should be replaced with your own Disqus site-id. -->
            

        </article>

    </div>
</main>

<!-- Links to Previous/Next posts -->
<aside class="read-next outer">
    <div class="inner">
        <div class="read-next-feed">
            
            
            
            
            
            

            <!-- If there's a next post, display it using the same markup included from - partials/post-card.hbs -->
            
            

<article class="post-card post-template">
    
    <a class="post-card-image-link" href="/coming-soon">
        <div class="post-card-image" style="background-image: url(/assets/images/welcome.jpg)"></div>
    </a>
    
    <div class="post-card-content">
        <a class="post-card-content-link" href="/coming-soon">
            <header class="post-card-header">
                

                <h2 class="post-card-title">Hello, welcome, and stay tune!</h2>
            </header>
            <section class="post-card-excerpt">
                
                <p></p>
                
            </section>
        </a>
        <footer class="post-card-meta">
            
            
            
            <img class="author-profile-image" src="/assets/images/ghost.png"
                alt="Aditya Kurniawan" />
            
            <span class="post-card-author">
                <a href="/about">Aditya Kurniawan</a>
            </span>
            
            
            <span class="reading-time">
                
                
                1 min read
                
            </span>
        </footer>
    </div>
</article>
            

            <!-- If there's a previous post, display it using the same markup included from - partials/post-card.hbs -->
            

        </div>
    </div>
</aside>

<!-- Floating header which appears on-scroll, included from includes/floating-header.hbs -->
<div class="floating-header">
    <div class="floating-header-logo">
        <a href="http://localhost:4000/">
            
            <span>Aditya Kurniawan</span>
        </a>
    </div>
    <span class="floating-header-divider">&mdash;</span>
    <div class="floating-header-title">Indonesian Language Model</div>
    <div class="floating-header-share">
        <div class="floating-header-share-label">Share this <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
    <path
        d="M7.5 15.5V4a1.5 1.5 0 1 1 3 0v4.5h2a1 1 0 0 1 1 1h2a1 1 0 0 1 1 1H18a1.5 1.5 0 0 1 1.5 1.5v3.099c0 .929-.13 1.854-.385 2.748L17.5 23.5h-9c-1.5-2-5.417-8.673-5.417-8.673a1.2 1.2 0 0 1 1.76-1.605L7.5 15.5zm6-6v2m-3-3.5v3.5m6-1v2" />
</svg></div>
        <a class="floating-header-share-tw"
            href="https://twitter.com/share?text=Indonesian+Language+Model&amp;url=https://akurniawan.github.io/indonesian-language-model"
            onclick="window.open(this.href, 'share-twitter', 'width=550,height=235');return false;">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32">
    <path
        d="M30.063 7.313c-.813 1.125-1.75 2.125-2.875 2.938v.75c0 1.563-.188 3.125-.688 4.625a15.088 15.088 0 0 1-2.063 4.438c-.875 1.438-2 2.688-3.25 3.813a15.015 15.015 0 0 1-4.625 2.563c-1.813.688-3.75 1-5.75 1-3.25 0-6.188-.875-8.875-2.625.438.063.875.125 1.375.125 2.688 0 5.063-.875 7.188-2.5-1.25 0-2.375-.375-3.375-1.125s-1.688-1.688-2.063-2.875c.438.063.813.125 1.125.125.5 0 1-.063 1.5-.25-1.313-.25-2.438-.938-3.313-1.938a5.673 5.673 0 0 1-1.313-3.688v-.063c.813.438 1.688.688 2.625.688a5.228 5.228 0 0 1-1.875-2c-.5-.875-.688-1.813-.688-2.75 0-1.063.25-2.063.75-2.938 1.438 1.75 3.188 3.188 5.25 4.25s4.313 1.688 6.688 1.813a5.579 5.579 0 0 1 1.5-5.438c1.125-1.125 2.5-1.688 4.125-1.688s3.063.625 4.188 1.813a11.48 11.48 0 0 0 3.688-1.375c-.438 1.375-1.313 2.438-2.563 3.188 1.125-.125 2.188-.438 3.313-.875z" />
    </svg>
        </a>
        <a class="floating-header-share-fb"
            href="https://www.facebook.com/sharer/sharer.php?u=https://akurniawan.github.io/indonesian-language-model"
            onclick="window.open(this.href, 'share-facebook','width=580,height=296');return false;">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32">
    <path d="M19 6h5V0h-5c-3.86 0-7 3.14-7 7v3H8v6h4v16h6V16h5l1-6h-6V7c0-.542.458-1 1-1z" /></svg>
        </a>
    </div>
    <progress class="progress" value="0">
        <div class="progress-container">
            <span class="progress-bar"></span>
        </div>
    </progress>
</div>

<!-- /post -->

<!-- The #contentFor helper here will send everything inside it up to the matching #block helper found in default.hbs -->

        <!-- Previous/next page links - displayed on every page -->
        

        <!-- The footer at the very bottom of the screen -->
        <footer class="site-footer outer">
            <div class="site-footer-content inner">
                <section class="copyright"><a href="http://localhost:4000/">Aditya Kurniawan</a> &copy;
                    2020</section>
                <section class="poweredby">Proudly published with <a href="https://jekyllrb.com/">Jekyll</a> &
                    <a href="https://pages.github.com/" target="_blank" rel="noopener">GitHub Pages</a> using
                    <a href="https://github.com/jekyller/jasper2" target="_blank" rel="noopener">Jasper2</a></section>
                <nav class="site-footer-nav">
                    <a href="/">Latest Posts</a>
                    
                    <a href="https://twitter.com/akurnia_1" target="_blank"
                        rel="noopener">Twitter</a>
                    <a href="https://github.com/akurniawan" target="_blank"
                        rel="noopener">Github</a>
                    <a href="https://linkedin.com/in/akurniawan25" target="_blank"
                        rel="noopener">LinkedIn</a>
                    <a href="mailto:akurniawan.cs@gmail.com" target="_blank"
                        rel="noopener">Email</a>
                    <!-- <a href="https://ghost.org" target="_blank" rel="noopener">Ghost</a> -->
                </nav>
            </div>
        </footer>

    </div>

    <!-- The big email subscribe modal content -->
    

    <!-- Add Google Analytics  -->
    <!-- Google Analytics Tracking code -->
<script>
    (function (i, s, o, g, r, a, m) {
        i['GoogleAnalyticsObject'] = r; i[r] = i[r] || function () {
            (i[r].q = i[r].q || []).push(arguments)
        }, i[r].l = 1 * new Date(); a = s.createElement(o),
            m = s.getElementsByTagName(o)[0]; a.async = 1; a.src = g; m.parentNode.insertBefore(a, m)
    })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');

    ga('create', 'UA-171726969-1', 'auto');
    ga('send', 'pageview');

</script>

    <!-- The #block helper will pull in data from the #contentFor other template files. In this case, there's some JavaScript which we only want to use in post.hbs, but it needs to be included down here, after jQuery has already loaded. -->
    
    <script>

    // NOTE: Scroll performance is poor in Safari
    // - this appears to be due to the events firing much more slowly in Safari.
    //   Dropping the scroll event and using only a raf loop results in smoother
    //   scrolling but continuous processing even when not scrolling
    $(document).ready(function () {
        // Start fitVids
        var $postContent = $(".post-full-content");
        $postContent.fitVids();
        // End fitVids

        var progressBar = document.querySelector('progress');
        var header = document.querySelector('.floating-header');
        var title = document.querySelector('.post-full-title');

        var lastScrollY = window.scrollY;
        var lastWindowHeight = window.innerHeight;
        var lastDocumentHeight = $(document).height();
        var ticking = false;

        function onScroll() {
            lastScrollY = window.scrollY;
            requestTick();
        }

        function onResize() {
            lastWindowHeight = window.innerHeight;
            lastDocumentHeight = $(document).height();
            requestTick();
        }

        function requestTick() {
            if (!ticking) {
                requestAnimationFrame(update);
            }
            ticking = true;
        }

        function update() {
            var trigger = title.getBoundingClientRect().top + window.scrollY;
            var triggerOffset = title.offsetHeight + 35;
            var progressMax = lastDocumentHeight - lastWindowHeight;

            // show/hide floating header
            if (lastScrollY >= trigger + triggerOffset) {
                header.classList.add('floating-active');
            } else {
                header.classList.remove('floating-active');
            }

            progressBar.setAttribute('max', progressMax);
            progressBar.setAttribute('value', lastScrollY);

            ticking = false;
        }

        window.addEventListener('scroll', onScroll, { passive: true });
        window.addEventListener('resize', onResize, false);

        update();
    });
</script>
    

    <!-- Ghost outputs important scripts and data with this tag - it should always be the very last thing before the closing body tag -->
    <!-- ghost_foot -->

</body>

</html>